---
title: 'Explainability of Large Language Model'
subtitle: ''

authors:

date: '2024-02-21'

publication_types: ['paper-conference']

# Publication name and optional abbreviated publication name.
tags: [research]

# Display this page in the Featured widget?
featured: true

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Illustration of how natural language queries from users are parsed into executable operations'
  focal_point: ''
  preview_only: false
---
Development of explanations (such as post-hoc explanations, causal reasoning, and Chain-of-Thought Prompting) for transparent AI models.
Human-centered XAI is prioritized to develop explanations that can be personalized for user needs at different levels of abstraction and detail.
Development of methods to verify model faithfulness, ensuring that explanations or predictions accurately reflect the actual internal decision-making process.
{style="color: grey"}
<!--keep-->
<!--three-->
<!--blank lines-->
* Wang, Qianli, et al. "Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem." arXiv preprint arXiv:2409.07123 (2024).

* Villa-Arenas, Luis Felipe, et al. "Anchored Alignment for Self-Explanations Enhancement." arXiv preprint arXiv:2410.13216 (2024).

* Wang, Qianli, et al. "FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation." arXiv preprint arXiv:2501.00777 (2025).